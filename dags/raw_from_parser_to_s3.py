import logging
import duckdb
import pendulum
from airflow import DAG
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.hooks.base import BaseHook
from airflow.exceptions import AirflowFailException

OWNER = "ilyas"
DAG_ID = "raw_from_parser_to_s3"

LAYER = "raw"

SHORT_DESCRIPTION = "DAG –∑–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è —Å–±–æ—Ä–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –æ –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ S3 –≤ —Ñ–æ—Ä–º–∞—Ç–µ .jsonl"

default_args = {
    'owner': OWNER,
    "start_date": pendulum.datetime(2026, 1, 18, tz="Europe/Moscow"),
    'retries': 2,
    "retry_delay": pendulum.duration(hours=2),
}


def check_raw_data_quality(**context) -> dict[str, int]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –≤ S3 —Å –ø–æ–º–æ—â—å—é duckdb"""
    s3_conn = BaseHook.get_connection("s3_conn")
    # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    access_key = s3_conn.login
    secret_key = s3_conn.password
    s3_endpoint = s3_conn.extra_dejson.get("endpoint_url")

    dt = context["data_interval_start"].in_timezone('Europe/Moscow')
    year = dt.year
    month = dt.strftime('%m')
    day = dt.strftime('%d')
    
    s3_path = f"s3://{LAYER}/cian/year={year}/month={month}/day={day}/flats.jsonl"

    con = duckdb.connect()
    con.execute(
        f"""
        INSTALL httpfs; -- —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å S3/HTTP
        LOAD httpfs;
        SET s3_url_style = 'path';
        SET s3_endpoint = '{s3_endpoint.replace("http://", "")}'; -- duckdb —Å–∞–º –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ—Ç http://
        SET s3_access_key_id = '{access_key}';
        SET s3_secret_access_key = '{secret_key}';
        SET s3_use_ssl = FALSE;
        """
    )

    logging.info("üíª –í—ã–ø–æ–ª–Ω—è—é –ø—Ä–æ–≤–µ—Ä–∫—É –¥–∞–Ω–Ω—ã—Ö")
    data_quality_results: tuple[int, int, int, int, int] = con.execute(
        f"""
            SELECT
                COUNT(*) as total_rows,
                COUNT(DISTINCT id) as unique_ids,
                COUNT(price) FILTER (WHERE price IS NOT NULL AND price != '') as valid_prices,
                COUNT(address) FILTER (WHERE address IS NOT NULL AND address != '') as valid_addresses,
                COUNT(metro) FILTER (WHERE metro IS NOT NULL AND metro != '') as valid_metro
            FROM read_json_auto('{s3_path}')
        """).fetchone()
    con.close()

    total_rows, unique_ids, valid_prices, valid_addresses, valid_metro = data_quality_results

    if total_rows == 0:
        raise AirflowFailException("–§–∞–π–ª –ø—É—Å—Ç–æ–π!")
    # —Ä–µ–π—Ç—ã
    unique_ids_rate: float = unique_ids / total_rows
    valid_prices_rate: float = valid_prices / total_rows
    valid_addresses_rate: float = valid_addresses / total_rows
    valid_metro_rate: float = valid_metro / total_rows
    # –ø—Ä–æ–≤–µ—Ä–∫–∏
    if unique_ids_rate < 0.90:
        logging.error(f"‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞. –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID - {unique_ids_rate:.2%}")
        raise AirflowFailException("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è ID!")
    
    if valid_prices_rate < 0.95:
        logging.error(f"‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞. –ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ü–µ–Ω - {valid_prices_rate:.2%}")
        raise AirflowFailException("C–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö —Ü–µ–Ω!")

    if valid_addresses_rate < 0.95:
        logging.error(f"‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞. –ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤ - {valid_addresses_rate:.2%}")
        raise AirflowFailException("C–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö –∞–¥—Ä–µ—Å–æ–≤!")

    logging.info(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows}. "
                 f"–ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∞–¥—Ä–µ—Å–æ–≤: {valid_addresses_rate:.2%}. " 
                 f"–ü—Ä–æ—Ü–µ–Ω—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–æ: {valid_metro_rate:.2%}")
    
    return {"total_rows": total_rows,
            "unique_ids": unique_ids,
            "valid_prices": valid_prices,
            "valid_addresses": valid_addresses}


with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 1 * * *",
    default_args=default_args,
    catchup=False,
    max_active_tasks=1,
    max_active_runs=1,
    tags=["s3", "raw"],
    description=SHORT_DESCRIPTION,
) as dag:

    start = EmptyOperator(
        task_id="start",
    )

    run_parser = DockerOperator(
        task_id='run_flats_parser',
        image='flats-parser:2.0',
        api_version='auto',
        auto_remove='success',
        docker_url="unix://var/run/docker.sock",
        network_mode="flats_analyze_default",
        mount_tmp_dir=False,
        tty=True, # –ª–æ–≥–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –≤–∏–¥–Ω—ã –≤ –ª–æ–≥–∞—Ö –∞–∏—Ä—Ñ–ª–æ—É
        mem_limit='4g',
        shm_size='2g',
        environment={
            'MINIO_ACCESS_KEY': "{{ conn.s3_conn.login }}",
            'MINIO_SECRET_KEY': "{{ conn.s3_conn.password }}",
            'MINIO_ENDPOINT_URL': "{{ conn.s3_conn.extra_dejson.endpoint_url }}",
            'MINIO_BUCKET_NAME': LAYER,
            'TZ': 'Europe/Moscow',
            'EXECUTION_DATE': "{{ data_interval_start.in_timezone('Europe/Moscow').format('YYYY-MM-DD') }}",
        }
    )

    check_data_quality = PythonOperator(
        task_id='check_data_quality',
        python_callable=check_raw_data_quality,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> run_parser >> check_data_quality >> end