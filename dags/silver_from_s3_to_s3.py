import logging
import duckdb
import pendulum
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.models import Variable
from airflow.sensors.external_task import ExternalTaskSensor

OWNER = "ilyas"
DAG_ID = "silver_from_s3_to_s3"

LAYER_SOURCE = "raw"
LAYER_TARGET = "silver"
SOURCE = "cian"

# S3
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")
S3_ENDPOINT = Variable.get("s3_endpoint")

SHORT_DESCRIPTION = "Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ ÑÑ‹Ñ€Ñ‹Ñ… JSONL Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ñ‚Ğ¸Ğ¿Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Parquet Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ DuckDB Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ² S3"

default_args = {
    'owner': OWNER,
    "start_date": pendulum.datetime(2026, 1, 18, tz="Europe/Moscow"),
    'retries': 2,
    "retry_delay": pendulum.duration(minutes=10),
}


def get_and_transform_raw_data_to_silver_s3(**context):
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ¿ÑƒÑ‚Ğ¸ Ğ´Ğ»Ñ duckdb
    dt = context["data_interval_start"].in_timezone('Europe/Moscow')
    year = dt.year
    month = dt.strftime('%m')
    day = dt.strftime('%d')
    
    raw_s3_path = f"s3://{LAYER_SOURCE}/{SOURCE}/year={year}/month={month}/day={day}/flats.jsonl"
    silver_s3_path = f"s3://{LAYER_TARGET}/{SOURCE}/year={year}/month={month}/day={day}/flats.parquet"

    con = duckdb.connect()
    # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğº S3
    connect_sql = f"""
        INSTALL httpfs;
        LOAD httpfs;
        SET s3_url_style = 'path';
        SET s3_endpoint = '{S3_ENDPOINT}';
        SET s3_access_key_id = '{ACCESS_KEY}';
        SET s3_secret_access_key = '{SECRET_KEY}';
        SET s3_use_ssl = FALSE;
    """
    con.execute(connect_sql)

    raw_count = con.execute(f"SELECT count(*) FROM read_json_auto('{raw_s3_path}')").fetchone()[0]
    logging.info(f"ğŸ“Š Ğ’Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (raw): {raw_count} ÑÑ‚Ñ€Ğ¾Ğº.")

    # Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ SQL Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°
    with open('dags/sql/transform_silver.sql', 'r') as f:
        sql_template = f.read()
    con.execute(
        sql_template.format(raw_path=raw_s3_path, silver_path=silver_s3_path)
    )

    silver_count = con.execute(f"SELECT count(*) FROM read_parquet('{silver_s3_path}')").fetchone()[0]
    logging.info(f"Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ÑĞ»Ğµ Ğ´ĞµĞ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (silver): {silver_count} ÑÑ‚Ñ€Ğ¾Ğº.")
    
    diff = raw_count - silver_count
    logging.info(f"Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¾ Ğ´ÑƒĞ±Ğ»ĞµĞ¹ Ğ¸ Ğ¼ÑƒÑĞ¾Ñ€Ğ°: {diff} ÑÑ‚Ñ€Ğ¾Ğº ({(diff/raw_count)*100:.2f}%).")

    con.close()

    logging.info(f"âœ… Ğ¤Ğ°Ğ¹Ğ» ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {silver_s3_path}")
    return {"raw_count": raw_count, "silver_count": silver_count, "removed": diff}


with DAG(
    dag_id=DAG_ID,
    schedule_interval="0 1 * * *",
    default_args=default_args,
    catchup=False,
    max_active_runs=1,
    tags=["s3", "silver"],
    description=SHORT_DESCRIPTION,
) as dag:

    start = EmptyOperator(
        task_id="start",
    )

    sensor_on_raw_layer = ExternalTaskSensor(
        task_id="sensor_on_raw_layer",
        external_dag_id="raw_from_parser_to_s3",
        allowed_states=["success"],
        mode="reschedule",
        timeout=36000,  # Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ ÑĞµĞ½ÑĞ¾Ñ€Ğ°
        poke_interval=60  # Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
    )

    transform_to_silver = PythonOperator(
        task_id="transform_to_silver",
        python_callable=get_and_transform_raw_data_to_silver_s3
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> sensor_on_raw_layer >> transform_to_silver >> end